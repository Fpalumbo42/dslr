# Logistic Regression Training - Complete Guide

A detailed explanation of logistic regression, gradient descent, and one-vs-all classification from first principles.

---

## Table of Contents

1. [Overview](#overview)
2. [What is Logistic Regression?](#what-is-logistic-regression)
3. [Mathematical Components](#mathematical-components)
4. [One-vs-All Strategy](#one-vs-all-strategy)
5. [Complete Training Process](#complete-training-process)
6. [Practical Examples](#practical-examples)

---

## Overview

**Goal**: Classify students into 4 Hogwarts houses based on their academic scores.

**Method**: Logistic Regression with One-vs-All strategy

**Key Requirements**:
- âœ… Use gradient descent (batch, not stochastic)
- âœ… Implement everything from scratch (no sklearn for training)
- âœ… Achieve â‰¥98% accuracy

---

## What is Logistic Regression?

### The Core Idea

Logistic regression transforms a **linear combination** of features into a **probability** (0 to 1).

```
Linear combination â†’ Sigmoid function â†’ Probability â†’ Classification
```

### Comparison with Linear Regression

**Linear Regression** (predicts continuous values):
```
Å· = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
Output: any value from -âˆ to +âˆ
```

**Logistic Regression** (predicts probabilities):
```
Å· = sigmoid(Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™)
Output: probability between 0 and 1
```

---

## Mathematical Components

### 1. Sigmoid Function

**Formula:**
```
Ïƒ(z) = 1 / (1 + e^(-z))
```

**What it does:**
- Input: any real number z (from -âˆ to +âˆ)
- Output: value between 0 and 1 (interpretable as probability)

**Properties:**
```
Ïƒ(0)    = 0.5      (neutral)
Ïƒ(+âˆ)   â†’ 1        (very confident: class 1)
Ïƒ(-âˆ)   â†’ 0        (very confident: class 0)
```

**Graph:**
```
  1.0 â”¤          â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚        â•­â•¯
  0.5 â”¤      â•­â•¯
      â”‚    â•­â•¯
  0.0 â”¤â”€â”€â•¯
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     -10  -5   0   5   10
```

**Examples:**
```python
Ïƒ(-5)  = 0.007    # 0.7% chance â†’ class 0
Ïƒ(-1)  = 0.27     # 27% chance â†’ class 0
Ïƒ(0)   = 0.5      # 50% chance â†’ uncertain
Ïƒ(1)   = 0.73     # 73% chance â†’ class 1
Ïƒ(5)   = 0.993    # 99.3% chance â†’ class 1
```

**Why sigmoid?**
1. **Bounded**: Always outputs values in [0, 1]
2. **Smooth**: Differentiable everywhere (needed for gradient descent)
3. **Probabilistic**: Can interpret output as P(y=1|x)
4. **Non-linear**: Can model curved decision boundaries

---

### 2. Hypothesis Function

**Formula:**
```
h_Î¸(x) = Ïƒ(Î¸áµ€x) = 1 / (1 + e^(-Î¸áµ€x))
```

**Breaking it down:**

**Step 1: Linear combination**
```
z = Î¸áµ€x = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™

where:
- Î¸ = weight vector [Î¸â‚€, Î¸â‚, Î¸â‚‚, ..., Î¸â‚™]
- x = feature vector [1, xâ‚, xâ‚‚, ..., xâ‚™]  (xâ‚€=1 is bias)
- z = weighted sum
```

**Step 2: Apply sigmoid**
```
h_Î¸(x) = Ïƒ(z)
```

**Interpretation:**
```
h_Î¸(x) â‰¥ 0.5  â†’  predict class 1
h_Î¸(x) < 0.5  â†’  predict class 0
```

**Concrete Example:**

Predict if a student is in Gryffindor:

```python
# Weights (learned during training)
Î¸ = [0.5, -0.8, 1.2]  # [bias, Î¸_Astronomy, Î¸_Herbology]

# Student's normalized features
x = [1, 0.6, -0.3]  # [bias=1, Astronomy=0.6, Herbology=-0.3]

# Step 1: Linear combination
z = 0.5Ã—1 + (-0.8)Ã—0.6 + 1.2Ã—(-0.3)
  = 0.5 - 0.48 - 0.36
  = -0.34

# Step 2: Sigmoid
h_Î¸(x) = 1 / (1 + e^(0.34))
       = 0.416

# Interpretation: 41.6% chance of Gryffindor
# Prediction: NOT Gryffindor (< 0.5)
```

---

### 3. Cost Function (Binary Cross-Entropy)

**Why do we need it?**

The cost function measures **how wrong** our predictions are. We want to find weights Î¸ that **minimize** this error.

**Formula:**
```
J(Î¸) = -(1/m) Ã— Î£áµ¢â‚Œâ‚áµ [yâ± log(h_Î¸(xâ±)) + (1-yâ±) log(1-h_Î¸(xâ±))]

where:
- m = number of training examples
- yâ± = actual label (0 or 1)
- h_Î¸(xâ±) = predicted probability
```

**For a single example:**

```
If y = 1 (student IS in this house):
  cost = -log(h_Î¸(x))

  h_Î¸(x) = 1.0  â†’  cost = 0       (perfect!)
  h_Î¸(x) = 0.9  â†’  cost = 0.105   (good)
  h_Î¸(x) = 0.5  â†’  cost = 0.693   (uncertain)
  h_Î¸(x) = 0.1  â†’  cost = 2.303   (bad)
  h_Î¸(x) = 0.0  â†’  cost = âˆ       (terrible!)

If y = 0 (student is NOT in this house):
  cost = -log(1 - h_Î¸(x))

  h_Î¸(x) = 0.0  â†’  cost = 0       (perfect!)
  h_Î¸(x) = 0.1  â†’  cost = 0.105   (good)
  h_Î¸(x) = 0.5  â†’  cost = 0.693   (uncertain)
  h_Î¸(x) = 0.9  â†’  cost = 2.303   (bad)
  h_Î¸(x) = 1.0  â†’  cost = âˆ       (terrible!)
```

**Key insight:**
- Cost is **0** for perfect predictions
- Cost is **high** for confident wrong predictions
- Cost is **always â‰¥ 0**

**Example with 3 students:**

```python
y = [1, 0, 1]      # Actual labels
h = [0.9, 0.2, 0.6]  # Predictions

# Student 0: y=1, h=0.9
costâ‚€ = -log(0.9) = 0.105

# Student 1: y=0, h=0.2
costâ‚ = -log(1-0.2) = -log(0.8) = 0.223

# Student 2: y=1, h=0.6
costâ‚‚ = -log(0.6) = 0.511

# Total cost
J(Î¸) = (0.105 + 0.223 + 0.511) / 3 = 0.280
```

**Why this formula?**
1. **Convex**: Single global minimum (no local minima)
2. **Differentiable**: We can compute gradients
3. **Heavily penalizes confident mistakes**: Forces model calibration

---

### 4. Gradient

**What is it?**

The gradient tells us **which direction to adjust each weight** to reduce the cost.

**Formula:**
```
âˆ‚J(Î¸)/âˆ‚Î¸â±¼ = (1/m) Ã— Î£áµ¢â‚Œâ‚áµ (h_Î¸(xâ±) - yâ±) Ã— xâ±¼â±
```

**Vectorized form (more efficient):**
```
âˆ‡J(Î¸) = (1/m) Ã— Xáµ€(h - y)

where:
- X = feature matrix (m Ã— n)
- h = predictions (m Ã— 1)
- y = labels (m Ã— 1)
- âˆ‡J(Î¸) = gradient vector (n Ã— 1)
```

**Intuition:**

Imagine the cost function as a 3D landscape. We want to find the lowest point (valley).

```
         â•±â•²
        â•±  â•²
       â•±    â•²
  â”€â”€â”€â”€â•±      â•²â”€â”€â”€â”€
     â•±        â•²
    â•±     â˜…    â•²    â† We're here
   â•±  (minimum) â•²
  â•±              â•²
```

The gradient points **uphill** (steepest increase).
We go **downhill** (opposite direction) to minimize cost.

**Example:**

```python
# 3 students, 2 features (+ bias)
X = [[1,  0.5, -0.3],   # Student 0
     [1, -0.2,  0.8],   # Student 1
     [1,  0.9,  0.4]]   # Student 2

y = [1, 0, 1]  # Labels

Î¸ = [0.1, 0.2, -0.5]  # Current weights

# Predictions
h = sigmoid(X @ Î¸) = [0.45, 0.40, 0.61]

# Errors
errors = h - y = [-0.55, 0.40, -0.39]

# Gradient for Î¸â‚€ (bias)
âˆ‚J/âˆ‚Î¸â‚€ = (1/3) Ã— (1Ã—(-0.55) + 1Ã—0.40 + 1Ã—(-0.39))
       = (1/3) Ã— (-0.54)
       = -0.18

# Gradient for Î¸â‚
âˆ‚J/âˆ‚Î¸â‚ = (1/3) Ã— (0.5Ã—(-0.55) + (-0.2)Ã—0.40 + 0.9Ã—(-0.39))
       = -0.235

# Gradient for Î¸â‚‚
âˆ‚J/âˆ‚Î¸â‚‚ = (1/3) Ã— ((-0.3)Ã—(-0.55) + 0.8Ã—0.40 + 0.4Ã—(-0.39))
       = 0.110

âˆ‡J(Î¸) = [-0.18, -0.235, 0.110]
```

**Interpretation:**
- `âˆ‚J/âˆ‚Î¸â‚€ = -0.18` â†’ Increase Î¸â‚€ to reduce cost (negative gradient)
- `âˆ‚J/âˆ‚Î¸â‚ = -0.235` â†’ Increase Î¸â‚ to reduce cost
- `âˆ‚J/âˆ‚Î¸â‚‚ = 0.110` â†’ Decrease Î¸â‚‚ to reduce cost (positive gradient)

---

### 5. Gradient Descent

**What is it?**

An iterative algorithm that adjusts weights to minimize the cost function.

**Algorithm:**
```
1. Initialize Î¸ = [0, 0, ..., 0]
2. Repeat for N iterations:
   a. Compute predictions: h = sigmoid(X @ Î¸)
   b. Compute gradient: âˆ‡J(Î¸) = (1/m) Ã— Xáµ€(h - y)
   c. Update weights: Î¸ := Î¸ - Î± Ã— âˆ‡J(Î¸)
   d. (Optional) Compute cost J(Î¸)
3. Return Î¸
```

**Update Rule:**
```
Î¸â±¼ := Î¸â±¼ - Î± Ã— âˆ‚J(Î¸)/âˆ‚Î¸â±¼

where Î± (alpha) = learning rate
```

**Learning Rate Î±:**

Controls the **step size**.

```
Î± too large:                Î± too small:              Î± just right:
  â•±â•²              â•±â•²           â•±â•²                        â•±â•²
 â•±  â•²    â˜…â†’â†â˜…    â•±  â•²         â•±  â•² â˜…                    â•±  â•²  â˜…
â•±    â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±    â•²       â•±    â•²â˜…                   â•±    â•²  â˜…
Overshoots!                  Very slow                        â˜…â˜…
                                                            Converges
```

**Typical values:** 0.01, 0.1, 0.3, 1.0

**Example (one iteration):**

```python
# Current weights
Î¸ = [0.1, 0.2, -0.5]

# Gradient (computed above)
âˆ‡J(Î¸) = [-0.18, -0.235, 0.110]

# Learning rate
Î± = 0.1

# Update
Î¸â‚€ = 0.1   - 0.1 Ã— (-0.18)  = 0.118
Î¸â‚ = 0.2   - 0.1 Ã— (-0.235) = 0.2235
Î¸â‚‚ = -0.5  - 0.1 Ã— 0.110    = -0.511

# New weights
Î¸ = [0.118, 0.2235, -0.511]

# Repeat 1000+ times until convergence
```

**Monitoring Progress:**

Track cost over iterations:

```
Iteration    0: J(Î¸) = 0.693  (random initialization)
Iteration  100: J(Î¸) = 0.420  (improving)
Iteration  500: J(Î¸) = 0.185  (improving)
Iteration 1000: J(Î¸) = 0.142  (converging)
Iteration 3000: J(Î¸) = 0.140  (converged!)
```

**Good sign:** Cost decreases monotonically âœ“
**Bad sign:** Cost increases or oscillates â†’ reduce Î±!

---

## One-vs-All Strategy

**Problem:** Logistic regression is binary (2 classes only).

**Our task:** Classify into 4 houses (multi-class).

**Solution:** Train **4 separate binary classifiers**.

### How It Works

**Training Phase:**

For each house h:
1. Create binary labels: `y_binary = 1 if house == h, else 0`
2. Train classifier Î¸_h using gradient descent
3. Store Î¸_h

**Example for Gryffindor:**

```python
Original:
Student 0: Gryffindor  â†’  y_binary = 1
Student 1: Hufflepuff  â†’  y_binary = 0
Student 2: Ravenclaw   â†’  y_binary = 0
Student 3: Gryffindor  â†’  y_binary = 1
Student 4: Slytherin   â†’  y_binary = 0

Train: Î¸_Gryffindor
```

**Prediction Phase:**

For a new student:
1. Compute probability for each house:
   ```python
   P(Gryffindor) = Ïƒ(Î¸_Gryffindor áµ€ x)
   P(Hufflepuff) = Ïƒ(Î¸_Hufflepuff áµ€ x)
   P(Ravenclaw)  = Ïƒ(Î¸_Ravenclaw áµ€ x)
   P(Slytherin)  = Ïƒ(Î¸_Slytherin áµ€ x)
   ```

2. Choose house with **highest probability**:
   ```python
   predicted_house = argmax(probabilities)
   ```

**Example:**

```python
x = [1, 0.8, -0.2, 0.5, 1.1]  # Student features

# Compute probabilities
P(Gryffindor) = Ïƒ(Î¸_G áµ€ x) = 0.77  â† Maximum!
P(Hufflepuff) = Ïƒ(Î¸_H áµ€ x) = 0.38
P(Ravenclaw)  = Ïƒ(Î¸_R áµ€ x) = 0.57
P(Slytherin)  = Ïƒ(Î¸_S áµ€ x) = 0.25

Prediction: Gryffindor (highest probability)
```

---

## Complete Training Process

### Full Pipeline

```
1. Load CSV
   â†“
2. Preprocessing:
   - Select features
   - Remove NaN rows
   - Normalize (z-score)
   - Add bias term
   â†“
3. One-vs-All Training:
   For each house h:
     - Create y_binary
     - Initialize Î¸_h = zeros
     - Run gradient descent
     - Store Î¸_h
   â†“
4. Save weights + normalization params
```

### Data Preprocessing

#### 1. Feature Selection

Choose features with good class separation:
```python
SELECTED_FEATURES = [
    'Astronomy',
    'Herbology',
    'Ancient Runes',
    'Divination',
    'Charms',
    'Flying'
]
```

**Why exclude some?**
- Defense Against the Dark Arts: correlated with Astronomy (râ‰ˆ0.99)
- Care of Magical Creatures: homogeneous distribution
- Arithmancy: no discriminative power

#### 2. Handle Missing Values

```python
# Drop rows with any NaN
mask = X.notna().all(axis=1) & y.notna()
X = X[mask]
y = y[mask]

# Result: 1600 â†’ ~1500 students
```

#### 3. Normalization (Z-score)

**Formula:**
```
x_normalized = (x - Î¼) / Ïƒ
```

**Example:**
```python
# Raw Astronomy scores
X['Astronomy'] = [5.2, 10.8, -3.1, 7.5]

# Compute stats
Î¼ = 5.0
Ïƒ = 3.0

# Normalize
X['Astronomy'][0] = (5.2 - 5.0) / 3.0 = 0.067
X['Astronomy'][1] = (10.8 - 5.0) / 3.0 = 1.93
X['Astronomy'][2] = (-3.1 - 5.0) / 3.0 = -2.7

# Result: mean â‰ˆ 0, std â‰ˆ 1
```

**Why normalize?**
1. **Faster convergence**: 10Ã— fewer iterations
2. **Numerical stability**: Prevents overflow
3. **Fair features**: All contribute equally

**CRITICAL:** Save Î¼ and Ïƒ for each feature!
```python
normalization_params = {
    'Astronomy': {'mean': 5.0, 'std': 3.0},
    'Herbology': {'mean': -2.1, 'std': 4.5},
    ...
}
```

You'll need these to normalize the test set!

#### 4. Add Bias Term

```python
# Before
X = [[0.5, -0.3, 1.2],
     [1.1,  0.5, -0.2]]

# After
X = [[1, 0.5, -0.3, 1.2],
     [1, 1.1,  0.5, -0.2]]
```

Why? Allows decision boundary to shift (not forced through origin).

---

## Practical Examples

### Example 1: Training Gryffindor Classifier

**Data:**
```python
X = [[1,  0.5, -0.3],   # 5 students
     [1, -0.8,  0.9],
     [1,  1.2,  0.4],
     [1, -0.2, -0.6],
     [1,  0.7,  1.1]]

y = ['Gryffindor', 'Hufflepuff', 'Gryffindor',
     'Ravenclaw', 'Gryffindor']

# Binary labels
y_binary = [1, 0, 1, 0, 1]
```

**Training:**
```python
Î¸ = [0, 0, 0]  # Initialize
Î± = 0.1

# Iteration 1
h = sigmoid(X @ Î¸) = [0.5, 0.5, 0.5, 0.5, 0.5]
J(Î¸) = 0.693

errors = h - y_binary = [-0.5, 0.5, -0.5, 0.5, -0.5]
âˆ‡J(Î¸) = [-0.1, -0.04, 0.12]

Î¸ = [0, 0, 0] - 0.1Ã—[-0.1, -0.04, 0.12]
  = [0.01, 0.004, -0.012]

# ... repeat 1000 times

# Final result
Î¸_Gryffindor = [0.35, 0.82, -0.61]
J(Î¸) = 0.12  â† Much better!
```

### Example 2: Making a Prediction

```python
# New student
x = [1, 0.6, -0.2]

# Probabilities (using trained weights)
P(Gryffindor) = Ïƒ([0.35, 0.82, -0.61] @ x) = 0.72
P(Hufflepuff) = Ïƒ([-0.2, -0.5, 0.9] @ x)   = 0.34
P(Ravenclaw)  = Ïƒ([0.1, 0.3, 0.5] @ x)     = 0.45
P(Slytherin)  = Ïƒ([-0.3, -0.4, -0.8] @ x)  = 0.28

# Prediction
max_prob = 0.72
house = "Gryffindor"
```

---

## Implementation Pseudocode

### Main Training Function

```python
def train():
    # 1. Load and preprocess
    X, y, norm_params = preprocess_data(df)

    # 2. Train one-vs-all
    houses = ['Gryffindor', 'Hufflepuff', 'Ravenclaw', 'Slytherin']
    all_thetas = {}

    for house in houses:
        # Binary labels
        y_binary = (y == house).astype(int)

        # Gradient descent
        theta = gradient_descent(X, y_binary, alpha=0.1, iterations=1000)

        # Store
        all_thetas[house] = theta

    # 3. Save weights
    save_weights(all_thetas, norm_params, 'weights.csv')
```

### Gradient Descent Function

```python
def gradient_descent(X, y, alpha=0.1, iterations=1000):
    m, n = X.shape
    theta = np.zeros(n)

    for i in range(iterations):
        # Predictions
        h = sigmoid(X @ theta)

        # Gradient
        gradient = (1/m) * (X.T @ (h - y))

        # Update
        theta = theta - alpha * gradient

        # Monitor
        if i % 100 == 0:
            cost = compute_cost(X, y, theta)
            print(f"Iteration {i}: Cost = {cost:.4f}")

    return theta
```

---

## Common Pitfalls & Solutions

| Problem | Symptom | Solution |
|---------|---------|----------|
| **No normalization** | Doesn't converge, cost oscillates | Apply z-score normalization |
| **Î± too large** | Cost increases | Reduce learning rate (try 0.01) |
| **Î± too small** | Very slow training | Increase learning rate (try 0.3) |
| **Forgot bias** | Poor performance | Add column of 1s to X |
| **log(0) error** | NaN in cost | Use `np.clip(h, 1e-7, 1-1e-7)` |
| **Wrong dimensions** | Shape mismatch | Check: X(m,n), y(m,), Î¸(n,) |
| **Not saving norm params** | Wrong predictions | Save Î¼, Ïƒ for test set |

---

## Summary

### Key Formulas

```
Sigmoid:        Ïƒ(z) = 1 / (1 + e^(-z))
Hypothesis:     h_Î¸(x) = Ïƒ(Î¸áµ€x)
Cost:           J(Î¸) = -(1/m)Î£[y log(h) + (1-y)log(1-h)]
Gradient:       âˆ‡J(Î¸) = (1/m)Xáµ€(h - y)
Update:         Î¸ := Î¸ - Î±âˆ‡J(Î¸)
Normalization:  x_norm = (x - Î¼) / Ïƒ
```

### Training Steps

```
1. Preprocess: Clean, normalize, add bias
2. For each house:
   - Create binary labels
   - Run gradient descent
   - Store weights
3. Save all weights + normalization params
```

### One-vs-All Prediction

```
1. For each house: compute P(house|x) = Ïƒ(Î¸_house áµ€ x)
2. Predict: argmax(probabilities)
```

---

**Now you understand the complete mathematics and process behind logistic regression training!** ğŸ“
