# DSLR - Data Science √ó Logistic Regression

> *Harry Potter and the Data Scientist*

Implement logistic regression from scratch to sort Hogwarts students into their houses.

**Goal**: Multi-class classifier using **one-vs-all logistic regression** with **gradient descent** to achieve ‚â•98% accuracy.

**Constraints**:
- ‚ùå Forbidden: mean, std, min, max, percentile, describe, etc.
- ‚ùå No sklearn for training (only accuracy_score for evaluation)
- ‚úÖ Implement everything manually

**Dataset**: 1600 students (training), 400 students (test), 13 course features, 4 houses

---

## Table of Contents

- [Part 1: Data Analysis](#part-1-data-analysis)
- [Part 2: Data Visualization](#part-2-data-visualization)
- [Part 3: Logistic Regression](#part-3-logistic-regression)
- [Usage](#usage)

---

## Part 1: Data Analysis

### `describe.py`

Display statistical information for all numerical features (like pandas `describe()`).

**Implemented Statistics**:

**Basic** (mandatory):
- **Count**: Number of non-null values
- **Mean**: `mean = (Œ£ xi) / n`
- **Std**: `std = ‚àö[Œ£(xi - mean)¬≤ / (n-1)]` ‚Üê Bessel's correction (n-1)
- **Min/Max**: Minimum and maximum values
- **Percentiles** (25%, 50%, 75%): Uses linear interpolation for non-integer positions

**Bonus**:
- Range, IQR, Variance, Skewness, Kurtosis, MAD, Outliers

---

## Part 2: Data Visualization

### Histogram Analysis

**Question**: *Which Hogwarts course has a homogeneous score distribution between all four houses?*

![Histogram Comparison](readme_images/histograms.png)

**Answer: Care of Magical Creatures**

**Why?**
- All four house histograms overlap significantly
- Distribution shapes are nearly identical across houses
- No house shows a distinctly different pattern
- ‚Üí Not useful for classification (all houses look the same)

**Key observations**:
- **Astronomy**: Excellent separation with distinct peaks per house ‚úÖ
- **Charms**: Good separation, especially Hufflepuff ‚úÖ
- **Arithmancy**: High overlap, poor discriminator ‚ùå

---

### Scatter Plot Analysis

**Question**: *What are the two features that are similar?*

**Pearson Correlation Formula**:
```
r = Œ£[(xi - xÃÑ)(yi - »≥)] / ‚àö[Œ£(xi - xÃÑ)¬≤ √ó Œ£(yi - »≥)¬≤]

where r ‚àà [-1, 1]:
  r = 1  ‚Üí perfect positive correlation
  r = -1 ‚Üí perfect negative correlation
  r = 0  ‚Üí no correlation
```

![Most Similar Features](readme_images/most_similar_features.png)

**Answer: Astronomy and Defense Against the Dark Arts (r ‚âà 0.99)**

**Why it matters**:
- Points form a clear diagonal line ‚Üí nearly perfect linear relationship
- **Multicollinearity problem**: Using both features makes the model unstable
- **Decision**: Keep only ONE (Astronomy OR Defense Against the Dark Arts)

---

### Pair Plot Analysis

**Question**: *From this visualization, which features are you going to use for your logistic regression?*

![Pair Plot Matrix](readme_images/pair_plot_matrix.png)

**Analysis method**:
- **Diagonal**: Histograms (distribution per house)
- **Off-diagonal**: Scatter plots (relationships between features)
- **Criteria**: Class separation quality + avoid redundancy

#### Feature-by-Feature Decision

| Feature | Quality | Decision | Reason |
|---------|---------|----------|--------|
| **Astronomy** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ KEEP | Excellent separation, distinct clusters |
| **Herbology** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ KEEP | Excellent separation |
| **Ancient Runes** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ KEEP | Very good horizontal separation |
| **Divination** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ KEEP | Clear clusters |
| **Charms** | ‚≠ê‚≠ê‚≠ê | ‚úÖ KEEP | Hufflepuff well separated |
| **Flying** | ‚≠ê‚≠ê‚≠ê | ‚úÖ KEEP | Gryffindor distinct |
| **Defense Against the Dark Arts** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå REMOVE | Correlated with Astronomy (r ‚âà 0.99) |
| **Care of Magical Creatures** | ‚≠ê‚≠ê | ‚ùå REMOVE | Homogeneous (from histogram) |
| **Arithmancy** | ‚≠ê | ‚ùå REMOVE | Complete mixing, no power |
| **Others** (Transfig, Muggle, Potions) | ‚≠ê‚≠ê | ‚ùå REMOVE | Too much overlap |

#### Final Selection: 6 Features

```
‚úÖ Astronomy        (excellent separator)
‚úÖ Herbology        (excellent separator)
‚úÖ Ancient Runes    (very good separator)
‚úÖ Divination       (very good separator)
‚úÖ Charms           (good separator)
‚úÖ Flying           (good separator, complements others)
```

**Why this works**:
- Each house has a unique "signature" across these 6 features
- No redundant features
- Expected performance: >98% accuracy

---

## Part 3: Logistic Regression

### Overview

**Strategy**: One-vs-All (train 4 binary classifiers, one per house)

```
1600 students √ó 6 features ‚Üí Train 4 classifiers ‚Üí Predict house (argmax probability)
```

---

### Step 1: Preprocessing

#### Normalization (Z-Score)

**Why?** Features have different scales (Astronomy ~500, Herbology ~1). Without normalization, gradient descent is slow.

**Formula**:
```
x_norm = (x - Œº) / œÉ
```

**Example** (real values from logreg_model.json):

Astronomy: Œº = 39.47, œÉ = 521.50
```
Score 600  ‚Üí x_norm = (600 - 39.47) / 521.50 = 1.075
Score -200 ‚Üí x_norm = (-200 - 39.47) / 521.50 = -0.459
```

Result: All features have **mean=0, std=1** ‚Üí fast convergence ‚úÖ

#### Bias Term

Add column of 1s:
```
Original:  [x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, x‚ÇÑ, x‚ÇÖ, x‚ÇÜ]
With bias: [1, x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, x‚ÇÑ, x‚ÇÖ, x‚ÇÜ]  ‚Üê 7 values
```

Purpose: Allows decision boundary to shift (not forced through origin)

---

### Step 2: One-vs-All Strategy

Train **4 separate binary classifiers**:

| Classifier | Question | Labels |
|------------|----------|--------|
| Œ∏_Gryffindor | "Is Gryffindor?" | Gryff=1, Others=0 |
| Œ∏_Hufflepuff | "Is Hufflepuff?" | Huff=1, Others=0 |
| Œ∏_Ravenclaw | "Is Ravenclaw?" | Rav=1, Others=0 |
| Œ∏_Slytherin | "Is Slytherin?" | Sly=1, Others=0 |

Each learns **Œ∏** with 7 weights (1 bias + 6 features)

---

### Step 3: Core Math

#### Sigmoid Function

**Formula**: `g(z) = 1 / (1 + e^(-z))`

**Purpose**: Convert any number to probability [0, 1]

**Examples**:
```
z = 0   ‚Üí 0.5    (50%)
z = 2   ‚Üí 0.88   (88%)
z = -2  ‚Üí 0.12   (12%)
z = 5   ‚Üí 0.993  (99%)
```

#### Hypothesis Function

**Formula**: `h(x) = sigmoid(Œ∏·µÄ ¬∑ x)`

**Example** (real trained weights):

Gryffindor Œ∏ = [-3.39, 1.27, -1.27, -2.23, 2.67]
Student x = [1, 0.5, -0.3, 1.2, 0.8]

```
z = Œ∏·µÄ¬∑x = (-3.39√ó1) + (1.27√ó0.5) + (-1.27√ó-0.3) + (-2.23√ó1.2) + (2.67√ó0.8)
z = -3.39 + 0.635 + 0.381 - 2.676 + 2.136 = -2.914

h(x) = 1/(1 + e^2.914) ‚âà 0.05  ‚Üí  5% Gryffindor
```

#### Cost Function (Binary Cross-Entropy)

**Formula**:
```
J(Œ∏) = -(1/m) √ó Œ£[y¬∑log(h(x)) + (1-y)¬∑log(1-h(x))]
```

where m=1600, y=label (0/1), h(x)=predicted probability

**Example** (3 students):

| Student | y | h(x) | Cost |
|---------|---|------|------|
| Harry | 1 | 0.95 | -log(0.95) = 0.051 |
| Hermione | 1 | 0.70 | -log(0.70) = 0.357 |
| Draco | 0 | 0.05 | -log(0.95) = 0.051 |

```
J(Œ∏) = -(1/3) √ó [0.051 + 0.357 + 0.051] = 0.153
```

**Interpretation**: Lower cost = better model, J(Œ∏)=0 = perfect

**Why log?** Penalizes confident wrong predictions heavily

---

### Step 4: Gradient Descent

#### Gradient

**Formula**: `‚àáJ(Œ∏) = (1/m) √ó X·µÄ ¬∑ (h - y)`

where:
- X = feature matrix (1600 √ó 7)
- h = predictions, y = labels

#### Update Rule

**Formula**: `Œ∏ := Œ∏ - Œ± √ó ‚àáJ(Œ∏)`

where Œ± = learning rate (0.1)

**Example** (1 weight):

```
Iter 0:    Œ∏=0, J=0.693, ‚àáJ=0.25  ‚Üí Œ∏_new = 0 - 0.1√ó0.25 = -0.025
Iter 1:    Œ∏=-0.025, J=0.680      ‚Üí Œ∏_new = -0.048
...
Iter 1000: Œ∏=-3.39, J=0.020       ‚Üí converged ‚úì
```

#### Early Stopping

Stop when: `|J_previous - J_current| < 1e-6`

```
Iter 0:    Cost = 0.6931
Iter 100:  Cost = 0.2451
Iter 500:  Cost = 0.0453
Iter 1200: Cost = 0.0201
Iter 1201: Cost = 0.0201  ‚Üê converged!
```

---

### Step 5: Making Predictions (logreg_predict.py)

#### 5.1 Loading the Model

The trained model is saved in JSON format with explicit feature-weight mapping:

```json
{
    "weights": {
        "Gryffindor": {
            "bias": -3.391,
            "Astronomy": 1.272,
            "Defense Against the Dark Arts": -1.272,
            "Herbology": -2.231,
            "Ancient Runes": 2.671
        },
        ...
    },
    "normalization_params": {
        "Astronomy": {"mean": 39.47, "std": 521.50},
        ...
    }
}
```

**Why this format?** Explicit feature names eliminate any risk of mixing up feature order between training and prediction.

#### 5.2 Preprocessing Test Data

**Step 1: Normalize features**

For each feature, apply z-score normalization using **training set** statistics (Œº and œÉ from model):

```python
x_norm = (x - Œº_train) / œÉ_train
```

**Important**: Never compute new statistics on test data - always use training set parameters!

**Step 2: Handle missing values**

Test dataset has NaN values. Strategy:

```python
# After normalization, replace NaN with 0
X = X.fillna(0)
```

**Why 0?** In normalized data:
- mean = 0, std = 1
- Setting NaN to 0 means "assume average value for this student"
- More neutral than dropping rows (which would give <400 predictions)

**Step 3: Add bias term**

```python
X.insert(0, 'bias', 1)
```

Result: `[1, x_astronomy, x_defense, x_herbology, x_runes]`

#### 5.3 Computing Probabilities

For **each student**, compute probability for **each house**:

**Real example** (student from test set):

Normalized features: `x = [1, 0.5, -0.3, 1.2, 0.8]` (bias + 4 features)

**Gryffindor**:
```
z = Œ∏·µÄ¬∑x = (-3.391√ó1) + (1.272√ó0.5) + (-1.272√ó-0.3) + (-2.231√ó1.2) + (2.671√ó0.8)
z = -3.391 + 0.636 + 0.382 - 2.677 + 2.137 = -2.913

P(Gryffindor) = sigmoid(-2.913) = 1/(1 + e^2.913) = 0.051  ‚Üí  5%
```

**Hufflepuff**:
```
z = (Œ∏_huff)·µÄ¬∑x = ... (same calculation with Hufflepuff weights)
P(Hufflepuff) = sigmoid(z) = 0.823  ‚Üí  82%
```

**Ravenclaw**: `P(Ravenclaw) = 0.095` ‚Üí 10%
**Slytherin**: `P(Slytherin) = 0.031` ‚Üí 3%

#### 5.4 Choosing the House (argmax)

**Manual argmax implementation** (max() is forbidden):

```python
def get_best_house(probabilities: dict) -> str:
    max_house = None
    max_proba = -float('inf')

    for house, proba in probabilities.items():
        if proba > max_proba:
            max_proba = proba
            max_house = house

    return max_house
```

**Result**: `max([0.051, 0.823, 0.095, 0.031])` = **Hufflepuff** ‚úÖ

#### 5.5 Output Format

Generate `houses.csv` with **exact format** required by subject:

```csv
Index,Hogwarts House
0,Gryffindor
1,Hufflepuff
2,Ravenclaw
3,Hufflepuff
...
399,Slytherin
```

**Critical**: Must have exactly 400 predictions with this format, or evaluation script will fail.

#### 5.6 Results

**Achieved accuracy: 99%** üéâ

```bash
$ python evaluate.py
Your score on test set: 0.99
```

Performance exceeds minimum requirement (98%) ‚úÖ

---

## Formula Summary

| Concept | Formula | Purpose |
|---------|---------|---------|
| **Normalization** | `(x - Œº) / œÉ` | Scale features |
| **Sigmoid** | `1 / (1 + e^(-z))` | ‚Üí probability [0,1] |
| **Hypothesis** | `h(x) = sigmoid(Œ∏·µÄx)` | Predict probability |
| **Cost** | `J(Œ∏) = -(1/m)Œ£[y¬∑log(h) + (1-y)¬∑log(1-h)]` | Measure error |
| **Gradient** | `‚àáJ = (1/m)X·µÄ(h - y)` | Update direction |
| **Update** | `Œ∏ := Œ∏ - Œ±¬∑‚àáJ` | Improve weights |
| **One-vs-All** | 4 binary classifiers | Handle 4 classes |
| **Argmax** | Loop to find max probability | Choose house |
| **NaN Handling** | `fillna(0)` after normalization | Assume average value |

---

## Usage

```bash
# Part 1: Data Analysis
python src/data/describe.py datasets/dataset_train.csv

# Part 2: Visualization
python src/data/histogram.py
python src/data/scatter_plot.py
python src/data/pair_plot.py

# Part 3: Training & Prediction
python src/logistic_regression/logreg_train.py datasets/dataset_train.csv
python src/logistic_regression/logreg_predict.py datasets/dataset_test.csv logreg_model.json

# Evaluation
python evaluate.py
```
